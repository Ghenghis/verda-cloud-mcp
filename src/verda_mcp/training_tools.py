"""Training Tools for Verda MCP Server.

Comprehensive training support including:
- Balance check before deployment
- Checkpoint script generation (PyTorch, HuggingFace, Lightning)
- Pre-configured startup scripts
- Cost alerts and monitoring
- Auto-resume from checkpoints
- Checkpoint upload to Google Drive
- Training notifications (webhooks)
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List
from dataclasses import dataclass, field
from pathlib import Path
import json

logger = logging.getLogger(__name__)


# =============================================================================
# CHECKPOINT SCRIPT TEMPLATES
# =============================================================================

PYTORCH_CHECKPOINT_TEMPLATE = '''#!/usr/bin/env python3
"""PyTorch Training Script with 10-Minute Checkpoints for Spot Instances.

Auto-generated by Verda MCP Server.
Features:
- Saves checkpoint every {checkpoint_minutes} minutes
- Auto-resumes from latest checkpoint
- Handles spot eviction gracefully
"""

import os
import time
import torch
import torch.nn as nn
from pathlib import Path
from datetime import datetime

# Configuration
CHECKPOINT_DIR = "{checkpoint_dir}"
CHECKPOINT_INTERVAL_SECONDS = {checkpoint_seconds}
MODEL_NAME = "{model_name}"

class CheckpointManager:
    """Manages checkpoints for spot instance training."""
    
    def __init__(self, checkpoint_dir: str = CHECKPOINT_DIR):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.last_checkpoint_time = time.time()
    
    def should_checkpoint(self) -> bool:
        """Check if it's time to save a checkpoint."""
        return (time.time() - self.last_checkpoint_time) >= CHECKPOINT_INTERVAL_SECONDS
    
    def save_checkpoint(self, model, optimizer, epoch: int, step: int, loss: float, **extra):
        """Save training checkpoint."""
        checkpoint = {{
            "model_state_dict": model.state_dict(),
            "optimizer_state_dict": optimizer.state_dict(),
            "epoch": epoch,
            "step": step,
            "loss": loss,
            "timestamp": datetime.now().isoformat(),
            **extra
        }}
        
        # Save with timestamp
        checkpoint_path = self.checkpoint_dir / f"checkpoint-step-{{step}}.pt"
        torch.save(checkpoint, checkpoint_path)
        
        # Also save as "latest" for easy resume
        latest_path = self.checkpoint_dir / "checkpoint-latest.pt"
        torch.save(checkpoint, latest_path)
        
        self.last_checkpoint_time = time.time()
        print(f"âœ… Checkpoint saved: {{checkpoint_path}} (step {{step}}, loss {{loss:.4f}})")
        
        return checkpoint_path
    
    def load_latest_checkpoint(self, model, optimizer):
        """Load the latest checkpoint if it exists."""
        latest_path = self.checkpoint_dir / "checkpoint-latest.pt"
        
        if latest_path.exists():
            checkpoint = torch.load(latest_path)
            model.load_state_dict(checkpoint["model_state_dict"])
            optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
            
            print(f"ðŸ”„ Resumed from checkpoint: step {{checkpoint['step']}}, epoch {{checkpoint['epoch']}}")
            return checkpoint
        
        print("ðŸ“ No checkpoint found, starting fresh")
        return None
    
    def cleanup_old_checkpoints(self, keep_last: int = 5):
        """Keep only the N most recent checkpoints."""
        checkpoints = sorted(
            self.checkpoint_dir.glob("checkpoint-step-*.pt"),
            key=lambda p: p.stat().st_mtime,
            reverse=True
        )
        
        for old_checkpoint in checkpoints[keep_last:]:
            old_checkpoint.unlink()
            print(f"ðŸ—‘ï¸ Removed old checkpoint: {{old_checkpoint.name}}")


# Example usage in training loop:
#
# checkpoint_mgr = CheckpointManager()
# start_epoch, start_step = 0, 0
# 
# # Try to resume
# checkpoint = checkpoint_mgr.load_latest_checkpoint(model, optimizer)
# if checkpoint:
#     start_epoch = checkpoint["epoch"]
#     start_step = checkpoint["step"]
# 
# for epoch in range(start_epoch, num_epochs):
#     for step, batch in enumerate(dataloader, start=start_step):
#         # ... training step ...
#         
#         # Check if time to checkpoint
#         if checkpoint_mgr.should_checkpoint():
#             checkpoint_mgr.save_checkpoint(model, optimizer, epoch, step, loss.item())
#             checkpoint_mgr.cleanup_old_checkpoints()
'''


HUGGINGFACE_CHECKPOINT_TEMPLATE = '''#!/usr/bin/env python3
"""HuggingFace Transformers Training with 10-Minute Checkpoints for Spot Instances.

Auto-generated by Verda MCP Server.
Features:
- Uses HuggingFace Trainer with checkpoint callbacks
- Saves every {checkpoint_minutes} minutes
- Auto-resumes from latest checkpoint
"""

import os
from pathlib import Path
from transformers import (
    Trainer,
    TrainingArguments,
    TrainerCallback,
    AutoModelForCausalLM,
    AutoTokenizer,
)
from datetime import datetime
import time

# Configuration
CHECKPOINT_DIR = "{checkpoint_dir}"
MODEL_NAME = "{model_name}"
CHECKPOINT_INTERVAL_SECONDS = {checkpoint_seconds}


class SpotCheckpointCallback(TrainerCallback):
    """Callback to save checkpoints every N minutes for spot instance safety."""
    
    def __init__(self, checkpoint_interval_seconds: int = CHECKPOINT_INTERVAL_SECONDS):
        self.checkpoint_interval = checkpoint_interval_seconds
        self.last_checkpoint_time = time.time()
    
    def on_step_end(self, args, state, control, **kwargs):
        if (time.time() - self.last_checkpoint_time) >= self.checkpoint_interval:
            control.should_save = True
            self.last_checkpoint_time = time.time()
            print(f"â° Time-based checkpoint triggered at step {{state.global_step}}")
        return control


def get_training_args(output_dir: str = CHECKPOINT_DIR) -> TrainingArguments:
    """Get training arguments optimized for spot instances."""
    return TrainingArguments(
        output_dir=output_dir,
        
        # Checkpoint settings - CRITICAL for spot!
        save_strategy="steps",
        save_steps=500,  # Also save every 500 steps as backup
        save_total_limit=5,  # Keep last 5 checkpoints
        
        # Resume from checkpoint
        resume_from_checkpoint=True,
        
        # Training settings
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        
        # Optimization
        learning_rate=2e-5,
        warmup_steps=100,
        weight_decay=0.01,
        
        # Logging
        logging_steps=10,
        logging_dir=f"{{output_dir}}/logs",
        
        # Mixed precision for GPU efficiency
        bf16=True,  # Use bf16 for Blackwell/Hopper GPUs
        
        # DataLoader
        dataloader_num_workers=4,
    )


def find_latest_checkpoint(output_dir: str) -> str:
    """Find the latest checkpoint directory."""
    output_path = Path(output_dir)
    checkpoints = sorted(
        output_path.glob("checkpoint-*"),
        key=lambda p: int(p.name.split("-")[-1]) if p.name.split("-")[-1].isdigit() else 0,
        reverse=True
    )
    
    if checkpoints:
        print(f"ðŸ”„ Found checkpoint: {{checkpoints[0]}}")
        return str(checkpoints[0])
    
    print("ðŸ“ No checkpoint found, starting fresh")
    return None


# Example usage:
#
# model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# 
# training_args = get_training_args()
# 
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     callbacks=[SpotCheckpointCallback()],
# )
# 
# # Auto-resume from checkpoint
# checkpoint = find_latest_checkpoint(CHECKPOINT_DIR)
# trainer.train(resume_from_checkpoint=checkpoint)
'''


LIGHTNING_CHECKPOINT_TEMPLATE = '''#!/usr/bin/env python3
"""PyTorch Lightning Training with 10-Minute Checkpoints for Spot Instances.

Auto-generated by Verda MCP Server.
Features:
- ModelCheckpoint callback with time-based saving
- Auto-resume from latest checkpoint
- Optimized for spot instance training
"""

import os
from pathlib import Path
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, Callback
from pytorch_lightning.loggers import TensorBoardLogger
import time

# Configuration
CHECKPOINT_DIR = "{checkpoint_dir}"
MODEL_NAME = "{model_name}"
CHECKPOINT_INTERVAL_SECONDS = {checkpoint_seconds}


class TimeBasedCheckpoint(Callback):
    """Save checkpoint every N minutes regardless of steps."""
    
    def __init__(self, checkpoint_dir: str, interval_seconds: int = CHECKPOINT_INTERVAL_SECONDS):
        super().__init__()
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
        self.interval_seconds = interval_seconds
        self.last_checkpoint_time = time.time()
    
    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        if (time.time() - self.last_checkpoint_time) >= self.interval_seconds:
            checkpoint_path = self.checkpoint_dir / f"time-checkpoint-step-{{trainer.global_step}}.ckpt"
            trainer.save_checkpoint(checkpoint_path)
            self.last_checkpoint_time = time.time()
            print(f"â° Time-based checkpoint saved: {{checkpoint_path}}")


def get_callbacks(checkpoint_dir: str = CHECKPOINT_DIR) -> list:
    """Get callbacks optimized for spot instances."""
    return [
        # Time-based checkpoint (every 10 minutes)
        TimeBasedCheckpoint(checkpoint_dir),
        
        # Step-based checkpoint (every 500 steps)
        ModelCheckpoint(
            dirpath=checkpoint_dir,
            filename="checkpoint-{{step}}-{{train_loss:.2f}}",
            save_top_k=5,
            every_n_train_steps=500,
            save_last=True,
        ),
        
        # Best model checkpoint
        ModelCheckpoint(
            dirpath=checkpoint_dir,
            filename="best-{{step}}-{{val_loss:.2f}}",
            save_top_k=1,
            monitor="val_loss",
            mode="min",
        ),
    ]


def find_latest_checkpoint(checkpoint_dir: str) -> str:
    """Find the latest checkpoint."""
    checkpoint_path = Path(checkpoint_dir)
    
    # Check for "last.ckpt" first
    last_ckpt = checkpoint_path / "last.ckpt"
    if last_ckpt.exists():
        print(f"ðŸ”„ Found last checkpoint: {{last_ckpt}}")
        return str(last_ckpt)
    
    # Find most recent checkpoint file
    checkpoints = sorted(
        checkpoint_path.glob("*.ckpt"),
        key=lambda p: p.stat().st_mtime,
        reverse=True
    )
    
    if checkpoints:
        print(f"ðŸ”„ Found checkpoint: {{checkpoints[0]}}")
        return str(checkpoints[0])
    
    print("ðŸ“ No checkpoint found, starting fresh")
    return None


# Example usage:
#
# trainer = pl.Trainer(
#     max_epochs=10,
#     accelerator="gpu",
#     devices=1,
#     precision="bf16-mixed",
#     callbacks=get_callbacks(),
#     logger=TensorBoardLogger(CHECKPOINT_DIR, name="logs"),
# )
# 
# # Auto-resume
# checkpoint = find_latest_checkpoint(CHECKPOINT_DIR)
# trainer.fit(model, datamodule, ckpt_path=checkpoint)
'''


# =============================================================================
# STARTUP SCRIPTS
# =============================================================================

STARTUP_SCRIPT_BASE = '''#!/bin/bash
# Verda Instance Startup Script
# Auto-generated by Verda MCP Server
# Framework: {framework}

set -e

echo "ðŸš€ Starting Verda Instance Setup..."
echo "ðŸ“… $(date)"

# Update system
apt-get update -qq

# Install common dependencies
apt-get install -y -qq git wget curl htop nvtop tmux screen

# Create workspace
mkdir -p /workspace
cd /workspace

# Install Python packages
pip install --upgrade pip

{framework_packages}

# Verify GPU
echo "ðŸ–¥ï¸ GPU Status:"
nvidia-smi

echo "âœ… Setup complete!"
echo "ðŸ“ Workspace: /workspace"
'''

FRAMEWORK_PACKAGES = {
    "pytorch": '''
pip install torch torchvision torchaudio
pip install transformers datasets accelerate
pip install wandb tensorboard
pip install bitsandbytes
''',
    "huggingface": '''
pip install torch torchvision torchaudio
pip install transformers[torch] datasets accelerate
pip install peft trl
pip install bitsandbytes flash-attn
pip install wandb tensorboard
pip install sentencepiece tokenizers
''',
    "lightning": '''
pip install torch torchvision torchaudio
pip install pytorch-lightning lightning
pip install transformers datasets
pip install wandb tensorboard
''',
    "llama": '''
pip install torch torchvision torchaudio
pip install transformers datasets accelerate
pip install peft trl bitsandbytes
pip install flash-attn --no-build-isolation
pip install wandb tensorboard
pip install sentencepiece unsloth
''',
    "stable_diffusion": '''
pip install torch torchvision torchaudio
pip install diffusers transformers accelerate
pip install safetensors xformers
pip install wandb tensorboard
''',
}


# =============================================================================
# CLASSES
# =============================================================================

@dataclass
class CostAlert:
    """Cost alert configuration."""
    threshold_usd: float
    alert_type: str = "warning"  # warning, critical, stop
    webhook_url: Optional[str] = None
    email: Optional[str] = None
    triggered: bool = False
    triggered_at: Optional[datetime] = None


class TrainingToolsManager:
    """Manages training tools and scripts."""
    
    def __init__(self):
        self.cost_alerts: List[CostAlert] = []
        self.active_notifications: Dict[str, str] = {}  # instance_id -> webhook_url
    
    def generate_checkpoint_script(
        self,
        framework: str = "pytorch",
        checkpoint_dir: str = "/workspace/checkpoints",
        checkpoint_minutes: int = 10,
        model_name: str = "my_model",
    ) -> str:
        """Generate a checkpoint-enabled training script."""
        checkpoint_seconds = checkpoint_minutes * 60
        
        templates = {
            "pytorch": PYTORCH_CHECKPOINT_TEMPLATE,
            "huggingface": HUGGINGFACE_CHECKPOINT_TEMPLATE,
            "hf": HUGGINGFACE_CHECKPOINT_TEMPLATE,
            "lightning": LIGHTNING_CHECKPOINT_TEMPLATE,
            "pl": LIGHTNING_CHECKPOINT_TEMPLATE,
        }
        
        template = templates.get(framework.lower(), PYTORCH_CHECKPOINT_TEMPLATE)
        
        return template.format(
            checkpoint_dir=checkpoint_dir,
            checkpoint_minutes=checkpoint_minutes,
            checkpoint_seconds=checkpoint_seconds,
            model_name=model_name,
        )
    
    def generate_startup_script(self, framework: str = "huggingface") -> str:
        """Generate a startup script for the specified framework."""
        framework_key = framework.lower()
        packages = FRAMEWORK_PACKAGES.get(framework_key, FRAMEWORK_PACKAGES["pytorch"])
        
        return STARTUP_SCRIPT_BASE.format(
            framework=framework,
            framework_packages=packages,
        )
    
    def add_cost_alert(
        self,
        threshold_usd: float,
        alert_type: str = "warning",
        webhook_url: str = None,
    ) -> CostAlert:
        """Add a cost alert."""
        alert = CostAlert(
            threshold_usd=threshold_usd,
            alert_type=alert_type,
            webhook_url=webhook_url,
        )
        self.cost_alerts.append(alert)
        return alert
    
    def check_cost_alerts(self, current_cost: float) -> List[CostAlert]:
        """Check if any cost alerts should be triggered."""
        triggered = []
        for alert in self.cost_alerts:
            if not alert.triggered and current_cost >= alert.threshold_usd:
                alert.triggered = True
                alert.triggered_at = datetime.now()
                triggered.append(alert)
        return triggered
    
    async def send_notification(
        self,
        webhook_url: str,
        message: str,
        event_type: str = "info",
    ) -> bool:
        """Send a notification via webhook."""
        try:
            import aiohttp
            
            payload = {
                "event": event_type,
                "message": message,
                "timestamp": datetime.now().isoformat(),
                "source": "verda-mcp",
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(webhook_url, json=payload) as resp:
                    return resp.status == 200
        except Exception as e:
            logger.error(f"Notification failed: {e}")
            return False
    
    def format_checkpoint_script(self, script: str, framework: str) -> str:
        """Format checkpoint script for display."""
        return f"""# ðŸ“ {framework.upper()} Checkpoint Script

Generated with 10-minute checkpoint intervals for spot instance safety.

## Features
- âœ… Auto-saves every 10 minutes
- âœ… Auto-resumes from latest checkpoint
- âœ… Keeps last 5 checkpoints
- âœ… Handles spot eviction gracefully

## Usage
1. Copy this script to your instance
2. Modify the training loop for your model
3. Run training - checkpoints are automatic!

---

```python
{script}
```
"""
    
    def format_startup_script(self, script: str, framework: str) -> str:
        """Format startup script for display."""
        return f"""# ðŸš€ {framework.upper()} Startup Script

Ready-to-use startup script for Verda instances.

## What It Installs
- Common tools (git, htop, nvtop, tmux)
- Python packages for {framework}
- GPU verification

## Usage
1. Create this as a Verda startup script
2. Attach to your instance deployment
3. Instance will auto-configure on boot

---

```bash
{script}
```
"""


# Global manager instance
_training_manager: Optional[TrainingToolsManager] = None


def get_training_manager() -> TrainingToolsManager:
    """Get the global training manager."""
    global _training_manager
    if _training_manager is None:
        _training_manager = TrainingToolsManager()
    return _training_manager


# =============================================================================
# ASYNC WRAPPER FUNCTIONS FOR MCP TOOLS
# =============================================================================

async def check_account_balance() -> str:
    """Check Verda account balance."""
    try:
        from .client import get_client
        client = get_client()
        
        # Note: This depends on Verda API having a balance endpoint
        # If not available, we'll estimate based on usage
        try:
            balance = await client.get_balance()
            return f"""# ðŸ’³ Account Balance

**Current Balance**: ${balance.get('balance', 'N/A')}
**Currency**: {balance.get('currency', 'USD')}

## Usage This Month
- **Compute**: ${balance.get('compute_usage', 0):.2f}
- **Storage**: ${balance.get('storage_usage', 0):.2f}
- **Total**: ${balance.get('total_usage', 0):.2f}

## Status
{'âœ… Sufficient funds' if balance.get('balance', 0) > 10 else 'âš ï¸ Low balance - please add funds'}
"""
        except AttributeError:
            # Fallback if balance API not available
            instances = await client.list_instances()
            running = [i for i in instances if i.status == "running"]
            
            return f"""# ðŸ’³ Account Status

âš ï¸ **Note**: Direct balance API not available. Contact Verda for balance info.

## Current Resources
- **Running Instances**: {len(running)}
- **Total Instances**: {len(instances)}

## To Check Balance
Visit: https://console.verda.com/dashboard/billing
"""
    except Exception as e:
        return f"âŒ Error checking balance: {e}"


async def generate_checkpoint_script(
    framework: str = "huggingface",
    checkpoint_dir: str = "/workspace/checkpoints",
    checkpoint_minutes: int = 10,
    model_name: str = "my_model",
) -> str:
    """Generate a checkpoint-enabled training script."""
    manager = get_training_manager()
    script = manager.generate_checkpoint_script(
        framework=framework,
        checkpoint_dir=checkpoint_dir,
        checkpoint_minutes=checkpoint_minutes,
        model_name=model_name,
    )
    return manager.format_checkpoint_script(script, framework)


async def generate_startup_script(framework: str = "huggingface") -> str:
    """Generate a startup script for the specified framework."""
    manager = get_training_manager()
    script = manager.generate_startup_script(framework)
    return manager.format_startup_script(script, framework)


async def set_cost_alert(
    threshold_usd: float,
    webhook_url: str = "",
) -> str:
    """Set a cost alert for training sessions."""
    manager = get_training_manager()
    alert = manager.add_cost_alert(
        threshold_usd=threshold_usd,
        alert_type="warning",
        webhook_url=webhook_url if webhook_url else None,
    )
    
    return f"""# ðŸ”” Cost Alert Set

**Threshold**: ${threshold_usd:.2f}
**Alert Type**: Warning
**Webhook**: {'Configured âœ…' if webhook_url else 'Not set'}

When training costs exceed ${threshold_usd:.2f}, you will be notified.

## Current Alerts
{len(manager.cost_alerts)} alert(s) configured
"""


async def send_training_notification(
    webhook_url: str,
    message: str,
    event_type: str = "training_update",
) -> str:
    """Send a training notification via webhook."""
    manager = get_training_manager()
    success = await manager.send_notification(webhook_url, message, event_type)
    
    if success:
        return f"âœ… Notification sent successfully!\n\n**Event**: {event_type}\n**Message**: {message}"
    else:
        return f"âŒ Failed to send notification. Check webhook URL."


async def upload_checkpoint_to_gdrive(
    instance_ip: str,
    checkpoint_path: str,
    gdrive_folder_id: str = "",
) -> str:
    """Upload a checkpoint from instance to Google Drive."""
    try:
        from .ssh_tools import get_ssh_manager
        
        manager = get_ssh_manager()
        loop = asyncio.get_event_loop()
        
        # First, check if gdown/gdrive is installed
        stdout, stderr, code = await loop.run_in_executor(
            None,
            lambda: manager.run_command(instance_ip, "which gdown || echo 'not found'")
        )
        
        if "not found" in stdout:
            # Install gdown
            await loop.run_in_executor(
                None,
                lambda: manager.run_command(instance_ip, "pip install gdown")
            )
        
        # For upload, we need gdrive CLI or rclone
        # Using rclone as it's more reliable for uploads
        stdout, stderr, code = await loop.run_in_executor(
            None,
            lambda: manager.run_command(instance_ip, "which rclone || echo 'not found'")
        )
        
        if "not found" in stdout:
            return f"""# âš ï¸ rclone Not Installed

To upload checkpoints to Google Drive, install rclone on the instance:

```bash
curl https://rclone.org/install.sh | sudo bash
rclone config  # Configure Google Drive
```

Then use:
```bash
rclone copy {checkpoint_path} gdrive:checkpoints/
```

Alternatively, use the checkpoint backup feature to download locally first.
"""
        
        # Upload using rclone
        folder = gdrive_folder_id if gdrive_folder_id else "verda-checkpoints"
        stdout, stderr, code = await loop.run_in_executor(
            None,
            lambda: manager.run_command(
                instance_ip,
                f"rclone copy {checkpoint_path} gdrive:{folder}/ --progress"
            )
        )
        
        if code == 0:
            return f"""# âœ… Checkpoint Uploaded to Google Drive

**Source**: {checkpoint_path}
**Destination**: gdrive:{folder}/

Upload complete! Your checkpoint is safely backed up.
"""
        else:
            return f"âŒ Upload failed: {stderr}"
            
    except Exception as e:
        return f"âŒ Error uploading checkpoint: {e}"


async def list_available_frameworks() -> str:
    """List available framework templates."""
    frameworks = {
        "pytorch": "Basic PyTorch with checkpoint manager",
        "huggingface": "HuggingFace Transformers with Trainer",
        "lightning": "PyTorch Lightning with callbacks",
        "llama": "LLaMA/Mistral fine-tuning with PEFT",
        "stable_diffusion": "Stable Diffusion with Diffusers",
    }
    
    lines = [
        "# ðŸ“š Available Framework Templates",
        "",
        "| Framework | Description |",
        "|-----------|-------------|",
    ]
    
    for name, desc in frameworks.items():
        lines.append(f"| `{name}` | {desc} |")
    
    lines.extend([
        "",
        "## Usage",
        "```",
        'generate_checkpoint_script(framework="huggingface")',
        'generate_startup_script(framework="llama")',
        "```",
        "",
        "All templates include:",
        "- âœ… 10-minute checkpoint saving",
        "- âœ… Auto-resume from latest checkpoint",
        "- âœ… Spot instance optimizations",
        "- âœ… bf16 mixed precision for modern GPUs",
    ])
    
    return "\n".join(lines)
